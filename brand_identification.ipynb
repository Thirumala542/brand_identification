{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "train_df=pd.read_csv('train.csv')\n",
    "test_df=pd.read_csv('test.csv')\n",
    "val_df=pd.read_csv('dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "\n",
    "def preprocess(df):\n",
    "    #Tokenization\n",
    "    tokenizer=Tokenizer(num_words=5000, split=\"\")\n",
    "    tokenizer.fit_on_texts(df['source'].values)\n",
    "    #Turning the tokenized text into sequences\n",
    "    X=tokenizer.texts_to_sequences (df['source'].values)\n",
    "    X=pad_sequences(X) # padding our text vector so they all have the same langth\n",
    "    #Label Encoding\n",
    "    le=LabelEncoder()\n",
    "    Y=le.fit_transform(df['target'])\n",
    "    Y=to_categorical (Y) #one-hot encoding\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train= preprocess(train_df)\n",
    "X_test, Y_test =preprocess(test_df)\n",
    "x_val, Y_val =preprocess(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model Definition\n",
    "def build_model(hp):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(5000, hp.Int(\"embedding_vector_length\", min_value=32, max_value=512, step=32), input_length = X_train.shape[1]))\n",
    "    model.add(LSTM(hp.Int('1stm_units', min_value=32, max_value=512, step=32), dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(hp.Int('dense_units', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "    model.add(Dense(Y_train.shape[1], activation='softmax')) # output layer size should match the number of classes\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "tuner=RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='brand_identification')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "tuner.search(X_train, Y_train, epochs=5, validation_data=(x_val,Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the optimal hyperparameters\n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters (num_trials=1)[0]\n",
    "\n",
    "print(f\"The hyperparameter search is complete. The optimal number of units in the LSTM layer is {best_hps.get('lstm_units')} and the optimal length of the embedding vector is {best_hps.get('embedding_vector_length')}.\")\n",
    "\n",
    "#Training the model with the optimal hyperparameters\n",
    "\n",
    "model=tuner.hypermodel.build(best_hps) \n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64, verbose=2, validation_data=(x_val, Y_val))\n",
    "\n",
    "#Saving the model\n",
    "\n",
    "model.save('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
